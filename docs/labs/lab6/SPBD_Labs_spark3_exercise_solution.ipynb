{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2324/blob/main/lab6/SPBD_Labs_spark3_exercise_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark SQL Exercises\n",
        "\n",
        "For this set of exercises, you should use SQL statements, as\n",
        "much as possible!\n",
        "\n",
        "Check this online resource for some help with [SQL queries](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/cheatsheet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c672b30-c854-4dfd-a66b-07b24e8fe6b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!wc os_maias.txt"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx",
        "outputId": "e9b8c5ce-ab50-4112-fd20-b4b00a26042e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   5877  216896 1292368 os_maias.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "1.1) Create a [Spark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n"
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB",
        "outputId": "efb929c7-eae3-491b-83f4-378b01cf4b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|word|freq|\n",
            "+----+----+\n",
            "|  de|8308|\n",
            "|   a|6720|\n",
            "|   o|6602|\n",
            "| que|4846|\n",
            "|   e|4441|\n",
            "|   -|3535|\n",
            "|  um|3004|\n",
            "| com|2792|\n",
            "|  do|2564|\n",
            "|  da|2200|\n",
            "| uma|2154|\n",
            "|  os|1762|\n",
            "|para|1733|\n",
            "|   E|1602|\n",
            "| não|1586|\n",
            "|  em|1505|\n",
            "|  no|1439|\n",
            "|  se|1427|\n",
            "|  as|1401|\n",
            "|  ao|1391|\n",
            "+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark Dataframes program that computes the top 10 most used words in \"Os Maias\" novel."
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC LIMIT 10\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF",
        "outputId": "62e72afa-3b94-4dcd-e52f-08864d3d20ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|word|freq|\n",
            "+----+----+\n",
            "|  de|8308|\n",
            "|   a|6720|\n",
            "|   o|6602|\n",
            "| que|4846|\n",
            "|   e|4441|\n",
            "|   -|3535|\n",
            "|  um|3004|\n",
            "| com|2792|\n",
            "|  do|2564|\n",
            "|  da|2200|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!wc web.log\n",
        "!head -1 web.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154bd86f-f295-4ab8-8126-a1d1bd528cf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  143457   860742 11758533 web.log\n",
            "2016-12-06T08:58:35.318+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.026  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Count the number of unique IP addresses involved in the DDOS attack.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0][0:18], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "#    x = spark.sql(\"SELECT count(*) FROM \\\n",
        "#                    (SELECT DISTINCT ipSource FROM WEBLOG)\")\n",
        "\n",
        "    x = spark.sql(\"SELECT count(DISTINCT ipSource) FROM WEBLOG\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-",
        "outputId": "cd435196-02b6-41c8-9a18-c670eb4f341a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+\n",
            "|count(DISTINCT ipSource)|\n",
            "+------------------------+\n",
            "|                     167|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT toInterval(date) as intervalo, count(*) as requests, \\\n",
        "                      min(time), max(time), mean(time) FROM WEBLOG \\\n",
        "                      GROUP BY intervalo \\\n",
        "                      ORDER BY requests DESC\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG",
        "outputId": "4730bede-ebec-47fc-a310-2c0bbe41dccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------+---------+---------+-------------------+\n",
            "|         intervalo|requests|min(time)|max(time)|         mean(time)|\n",
            "+------------------+--------+---------+---------+-------------------+\n",
            "|2016-12-06T09:00:4|   12135|    0.007|    9.537|0.32576440049443844|\n",
            "|2016-12-06T09:01:1|   10552|    0.004|    1.506|0.14804018195602886|\n",
            "|2016-12-06T09:00:1|    9719|    0.225|   34.406|  7.857372672085602|\n",
            "|2016-12-06T09:01:0|    8747|    0.005|    1.624|0.17546667428832857|\n",
            "|2016-12-06T09:00:5|    8062|    0.006|    1.905|0.20009092036715637|\n",
            "|2016-12-06T08:59:3|    8015|    0.056|   67.441| 11.210152214597631|\n",
            "|2016-12-06T08:59:4|    7947|    0.914|   65.706|  7.761815779539431|\n",
            "|2016-12-06T08:59:0|    6914|    0.018|   81.659|  38.53438212322824|\n",
            "|2016-12-06T09:00:0|    6882|    0.017|   45.314|  8.649971519907023|\n",
            "|2016-12-06T09:00:3|    6771|    0.007|    26.53| 1.6047638458130256|\n",
            "|2016-12-06T09:00:2|    6616|    0.014|   25.847|  4.611345223700128|\n",
            "|2016-12-06T08:59:1|    6271|    0.017|   83.993|  32.96384978472328|\n",
            "|2016-12-06T09:01:3|    6163|    0.005|    1.117|0.11656384877494576|\n",
            "|2016-12-06T08:59:5|    5983|    0.678|    54.29|  3.821664382416849|\n",
            "|2016-12-06T08:58:5|    5500|    0.017|   80.846|  38.52511163636371|\n",
            "|2016-12-06T08:59:2|    5434|    0.051|   77.967|  17.29333143172616|\n",
            "|2016-12-06T09:01:2|    5315|    0.005|    1.361| 0.1536705550329246|\n",
            "|2016-12-06T09:01:4|    5294|    0.005|    1.339| 0.1001936154136758|\n",
            "|2016-12-06T09:01:5|    3343|    0.005|    1.098| 0.0984113072090947|\n",
            "|2016-12-06T08:58:4|    2611|    0.014|   69.654| 30.159845653006503|\n",
            "+------------------+--------+---------+---------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0][0:18], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows )\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT toInterval(date) as intervalo, collect_set( ipSource) as Ips FROM WEBLOG \\\n",
        "                          GROUP BY intervalo \\\n",
        "                          ORDER BY intervalo DESC\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "RpXghha0C0jC",
        "outputId": "af195e59-3d1d-4dc8-d3db-2ccb8d6a5fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+\n",
            "|         intervalo|                 Ips|\n",
            "+------------------+--------------------+\n",
            "|2016-12-06T10:03:2|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:03:1|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:03:0|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:02:5|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:02:4|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:02:3|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:02:2|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:02:1|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:02:0|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:01:5|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:01:4|[106.37.189.69, 1...|\n",
            "|2016-12-06T10:01:3|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:01:2|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:01:1|[106.37.189.69, 2...|\n",
            "|2016-12-06T10:01:0|[106.37.189.69, 2...|\n",
            "|2016-12-06T09:55:3|    [88.157.128.134]|\n",
            "|2016-12-06T09:32:2|     [64.74.215.139]|\n",
            "|2016-12-06T09:27:0|[106.37.189.69, 2...|\n",
            "|2016-12-06T09:26:5|[106.37.189.69, 2...|\n",
            "|2016-12-06T09:26:4|[106.37.189.69, 2...|\n",
            "+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4. Create an inverted index that, for each interval of 15 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "T6W21dG8R2MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    lines = sc.textFile('web.log')\n",
        "    logRows = lines.filter( lambda line : len(line) > 0 ) \\\n",
        "                   .map( lambda line : line.split(' ') ) \\\n",
        "                   .map( lambda l : Row( date = l[0], \\\n",
        "\t\t\t\t    \t\t            ipSource = l[1], retValue = l[2], \\\n",
        "                            op = l[3], url = l[4], time = float(l[5])))\n",
        "\n",
        "    logRowsDF = spark.createDataFrame( logRows ).withColumn('date', col('date').cast(\"timestamp\"))\n",
        "    logRowsDF.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    x = spark.sql(\"SELECT from_unixtime((unix_timestamp(date) div 15) * 15) as intervalo, collect_set(ipSource) as Ips \\\n",
        "                      FROM WEBLOG GROUP BY intervalo ORDER BY intervalo\")\n",
        "\n",
        "    x.show()\n",
        "    sc.stop()\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    sc.stop()"
      ],
      "metadata": {
        "id": "f-OjejqGR8W9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4765ef1-d6e2-44de-aacb-8f8ca7ce73f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+\n",
            "|          intervalo|                 Ips|\n",
            "+-------------------+--------------------+\n",
            "|2016-12-06 08:58:30|[202.106.16.36, 2...|\n",
            "|2016-12-06 08:58:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:00|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:15|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:30|[2a01:488:66:1000...|\n",
            "|2016-12-06 08:59:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:00|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:15|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:30|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:00:45|[2a01:488:66:1000...|\n",
            "|2016-12-06 09:01:00|[187.60.170.22, 1...|\n",
            "|2016-12-06 09:01:15|[2001:41d0:8:11c6...|\n",
            "|2016-12-06 09:01:30|[114.215.150.13, ...|\n",
            "|2016-12-06 09:01:45|[103.18.4.13, 120...|\n",
            "|2016-12-06 09:02:00|[120.55.83.30, 18...|\n",
            "|2016-12-06 09:02:15|[185.15.43.51, 20...|\n",
            "|2016-12-06 09:02:30|[120.52.73.97, 17...|\n",
            "|2016-12-06 09:02:45|[177.54.250.18, 1...|\n",
            "|2016-12-06 09:03:15|      [185.15.42.51]|\n",
            "|2016-12-06 09:03:30|      [185.15.42.51]|\n",
            "+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}