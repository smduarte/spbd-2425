{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2425/blob/main/docs/labs/lab9/SPBD_Labs_spark4_exercise_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Streaming Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuFS4wO2B1vr"
      },
      "outputs": [],
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7BidJJE_lW1"
      },
      "source": [
        "---\n",
        "# Exercises\n",
        "\n",
        "**Every 3 seconds**,\n",
        "1. Dump the number of requests in the last 10 seconds;\n",
        "2. Dump the number of requests in the last 10 seconds, only if they total more than 100;\n",
        "3. Dump the number of requests in the last 10 seconds, if there is an IP address with more than 100 requests;\n",
        "4. Dump the proportion of IPv4 vs IPv6 requests in the last 20 seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izpS6n8W_5vz"
      },
      "source": [
        "# Structured Spark Streaming\n",
        "\n",
        "The python code below shows the basics needed to process data from socket source using PySpark.\n",
        "\n",
        "Spark Structured Streaming (Spark SQL Streaming) python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qVr9MSlIlsg"
      },
      "outputs": [],
      "source": [
        "#@title Start the Structured Source\n",
        "\n",
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/json_logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python json_logsender/server.py json_logsender/web.log 8888 > /dev/null 2> /dev/null &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH_2A7kOASSx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "\n",
        "  query = json_lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  query.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCV88xx9Dd1i"
      },
      "source": [
        "---\n",
        "# Exercises\n",
        "\n",
        "Do the follwing exercises:\n",
        "\n",
        "**Every 3 seconds**,\n",
        "1. Dump the number of requests in the last 10 seconds;\n",
        "2. Dump the number of requests in the last 10 seconds, only if they total more than 100;\n",
        "3. Dump the number of requests in the last 10 seconds, if there is an IP address with more than 100 requests;\n",
        "4. Dump the proportion of IPv4 vs IPv6 requests in the last 20 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088m_Z3i8kHD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Q1\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "  json_lines.printSchema()\n",
        "\n",
        "  requests = json_lines \\\n",
        "      .withColumn(\"date\", to_timestamp(col(\"timestamp\"))) \\\n",
        "      .withWatermark(\"date\", \"0 seconds\") \\\n",
        "      .groupBy(window(\"date\", \"10 seconds\")).count() \\\n",
        "      .withColumnRenamed(\"count\", \"#requests\") \\\n",
        "      .orderBy('window', ascending = False).limit(1) \\\n",
        "\n",
        "  query = requests \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='3 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(160)\n",
        "except Exception as err:\n",
        "  print(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvnMITpQCOfV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Q2\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "  json_lines.printSchema()\n",
        "\n",
        "  requests = json_lines \\\n",
        "      .withColumn(\"date\", to_timestamp(col(\"timestamp\"))) \\\n",
        "      .withWatermark(\"date\", \"0 seconds\") \\\n",
        "      .groupBy(window(\"date\", \"10 seconds\")).count() \\\n",
        "      .where(\"count > 100\") \\\n",
        "      .withColumnRenamed(\"count\", \"#requests\") \\\n",
        "      .orderBy('window', ascending = False).limit(1) \\\n",
        "\n",
        "  query = requests \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='3 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(120)\n",
        "except Exception as err:\n",
        "  print(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9T-oaLoCyyz"
      },
      "outputs": [],
      "source": [
        "#@title Q3\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .config('spark.sql.streaming.statefulOperator.checkCorrectness.enabled','False')\\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "  json_lines.printSchema()\n",
        "\n",
        "  requests = json_lines \\\n",
        "      .withColumn(\"date\", to_timestamp(col(\"timestamp\"))) \\\n",
        "      .withWatermark(\"date\", \"0 seconds\") \\\n",
        "\n",
        "  requests = (requests \\\n",
        "      .groupBy(window(\"date\", \"10 seconds\"), \"ip\").count()\n",
        "      .groupBy('window').agg(sum('count').alias('#requests'), max('count').alias('max_requests_by_ip'))\n",
        "      .where(\"max_requests_by_ip > 100\")\n",
        "      .orderBy('window', ascending = False).limit(1)\n",
        "  )\n",
        "\n",
        "  query = requests \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='3 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(300)\n",
        "except Exception as err:\n",
        "  print(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG06h2ybMg-C"
      },
      "outputs": [],
      "source": [
        "#@title Q4\n",
        "\n",
        "def processResult(df, epoch):\n",
        "\n",
        "    df = df.groupBy('window') \\\n",
        "          .pivot('ip_version', ['v4', 'v6']).agg(sum(\"count\").cast('double')) \\\n",
        "          .fillna(0) \\\n",
        "          .orderBy('window', ascending = False).limit(1)\n",
        "\n",
        "    df = df.selectExpr('window', 'v4', 'v4 / (v4+v6) as v4pc', 'v6', 'v6 / (v4+v6) as v6pc')\n",
        "    df.show(1000, truncate=False)\n",
        "\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "  json_lines.printSchema()\n",
        "\n",
        "  requests = json_lines \\\n",
        "            .withColumn(\"ip_version\",\n",
        "              when(col(\"ip\").contains('.'), \"v4\")\n",
        "              .when(col(\"ip\").contains(':'), \"v6\").otherwise(\"?\")\n",
        "  )\n",
        "\n",
        "  requests = requests \\\n",
        "      .withColumn(\"date\", to_timestamp(col(\"timestamp\"))) \\\n",
        "      .withWatermark(\"date\", \"0 seconds\") \\\n",
        "      .groupBy(window(\"date\", \"20 seconds\"), 'ip_version').count() \\\n",
        "\n",
        "  query = requests \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='3 seconds') \\\n",
        "    .foreachBatch( processResult ) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(360)\n",
        "except Exception as err:\n",
        "  print(err)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}